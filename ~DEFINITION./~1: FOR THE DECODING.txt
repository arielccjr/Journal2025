~COMPUTER-SCIENCE: REASONING
    ~1: SEARCH/Queries: Retrieves specific data from the database. To access and analyze stored information.
        ~1: Define Initial State: Start with a known state.
        ~2: Check Goal State: Verify if the current state meets the goal criteria.
        ~3: Expand Nodes: Generate possible next states from the current state.
        ~4: Store in Frontier: Maintain a list of unexplored nodes.
            : DEPTH-FIRST SEARCH: Push Initial State into Stack Frontier.
            : BREADTH-FIRST SEARCH: Enqueue Initial State into Queue Frontier.
            : A* SEARCH: Initialize Priority Queue Frontier with Initial State.
        ~5: Use Search Strategy: Choose the next state based on the algorithm.
            : DEPTH-FIRST SEARCH: Loop until Solution or Empty Frontier:
                : Pop the last added node.
                : If goal state, return solution.
                : Otherwise, expand node and push new nodes into the stack.
                : Backtrack when dead ends are reached.
            : BREADTH-FIRST SEARCH: Loop until Solution or Empty Frontier:
                : Dequeue the first added node.
                : If goal state, return solution.
                : Otherwise, expand node and enqueue new nodes.
            : A* SEARCH: Loop until Solution or Empty Frontier:
                : Remove the node with the lowest f(n) = g(n) + h(n).
                : If goal state, return solution.
                : Otherwise, expand node and add new nodes with updated f(n).    
            : MINIMAX ALGORITHM (For Adversarial Search) 
                ~1: Recursively Explore Possible Moves.
                ~2: If Terminal State, Return Utility.
                ~3: If Max Player, Choose Move with Maximum Utility.
                ~4: If Min Player, Choose Move with Minimum Utility. 
       ~6: Avoid Infinite Loops: Track visited states to prevent redundant exploration.

    ~4: OPTIMIZATION: the process of selecting the best option from a set of possible choices 
        to achieve a specific goal efficiently.
        : Hill Climbing: algorithm follows a simple logic:
            ~1: Start with an initial state.
            ~2: Evaluate neighboring states.
            ~3: Move to the best neighbor if it improves the solution.
            ~4: Repeat until no better neighbor exists.
        : Simulated Annealing: introduces randomness in the search process by sometimes accepting worse states, with decreasing probability over time.
        : Linear Programming is used for problems where constraints must be satisfied while minimizing or maximizing a cost function.
    ~5: LEARNING: 
        : Supervised learning = given a data set of input-output pairs, learn a function to map inputs to outputs
            ~1: Prepare Data: Collect labeled input-output pairs.
            ~2: Split Data: Divide into training and testing sets.
            ~3: Train Model: Fit a hypothesis function to the training data.
            ~4: Evaluate Model: Test the trained model on unseen data.
            ~5: Adjust Weights: Update parameters based on accuracy.
        : Nearest Neighbor Classification: A simple supervised learning method where classification is based on the closest labeled data points: 
            ~1: Store all training data.
            ~2: For a new input:
                : Compute distance to all stored points.
                : Select the closest k neighbors.
                : Assign the most common class among neighbors.
            ~3: Output the predicted class.
        : Perceptron Learning: A linear model that adjusts weights iteratively based on errors in classification:
            ~1: Initialize weights randomly.
            ~2: For each data point (x, y):
                : Compute prediction using dot product.
                : Compare prediction with actual output.
                : If incorrect, adjust weights: wi=wi+α(y−y^)xi
            ~3: Repeat until convergence.
        : reinforcement learning = given a set of rewards or punishments, learn what actions to take in the future
        : unsupervised learning = given input data without any additional feedback, learn patterns

~TRANSFORMERS ARCHITECTURE for NATURAL LANGUAGE PROCESSING:
    : Encoder: input word + positional encoding >>> (multi-head self attention >>> neural network) * Number >>> encoded representation
    : Decoder: previous output word + positional encoding >>> (multi-head self attention >>> (encoded representations) attention >>> neural network) * Number >>> encoded representation
    ~1: Encoder-Decoder Architecture
        : Encoder: Processes the input sentence into a fixed representation.
        : Decoder: Generates the output sequence, one word at a time.
    ~2: Self-Attention Mechanism
        Instead of using RNNs, the Transformer calculates attention scores for all words in a sentence simultaneously.
        Multi-Head Attention enables different aspects of word relationships to be captured.
    ~3: Positional Encoding: Since Transformers do not process sequences sequentially, positional encoding is added to retain information about word order.
    ~4: Feed-Forward Layers: Each word's representation is passed through fully connected layers to extract deeper features.
    ~5: Masked Self-Attention in the Decoder: Prevents words from attending to future words (maintains causality).
    ~6: Output Prediction: The decoder predicts the next word using a softmax function, generating the final output.
    ~7: Optimization and Training
        Uses the Adam optimizer with learning rate scheduling.
        Dropout and Label Smoothing are applied for regularization.
        Beam Search is used for better sentence generation.

~ARISTOTLE'S THE ORGANON: DEDUCTIVE REASONING
    : CATEGORIES: classifies different kinds of things that can be said about a subject.
    : ON INTERPRETATION (Peri Hermeneias): It focuses on how statements (propositions) express truth and falsity and lays the foundation for modal logic, contradiction, and necessity.
    : PRIOR ANALYTICS: explains how conclusions necessarily follow from given premises when structured correctly.
    : POSTERIOR ANALYTICS: develops a theory of scientific knowledge (epistēmē) to explain how demonstrative knowledge (apodeixis) is structured.
    : TOPICS: focusing on how one can argue both for and against a proposition.
    : SOPHISTICAL ELENCHI: primarily focusing on eristic (contentious) arguments used in debate 